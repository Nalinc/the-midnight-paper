Notes and summaries from papers I read(usually at midnight)

# List of papers
**Computer Vision in HCI**
- ["Eyepatch: Prototyping camera based interaction through examples"](https://nalinc.github.io/the-midnight-paper/papers/Eyepatch_Prototyping_Camera_based_Interact_on_through_examples)
- ["Robust Computer Vision-Based Detection of Pinching for One and Two-Handed Gesture Input"](https://nalinc.github.io/the-midnight-paper/papers/Robust_CV_Based_Detection_of_Pinching)
- ["Cyclops: Wearable and Single-Piece Full-Body Gesture Input Devices"](https://nalinc.github.io/the-midnight-paper/papers/Cyclops:Full_Body_Gesture_Input_Devices)
- ["OmniTouch: Wearable Multitouch Interaction Everywhere"](https://nalinc.github.io/the-midnight-paper/papers/OmniTouch:Wearable_Multitouch_Interaction_Everywhere)
- ["Bonfire: A Nomadic System for Hybrid Laptop-Tabletop Interaction"](https://nalinc.github.io/the-midnight-paper/papers/Bonfire:A_Nomadic_System_for_Hybrid_Laptop-Tabletop_Interaction)
- ["Surround-See: Enabling Peripheral Vision on Smartphones during Active Use"](https://nalinc.github.io/the-midnight-paper/papers/Surround-See:Enabling_Peripheral_Vision_on_Smartphones_during_Active_Use)
- ["VisionWand: Interaction Techniques for Large Displays using a Passive Wand Tracked in 3D"](https://nalinc.github.io/the-midnight-paper/papers/VisionWand:Interaction_Techniques_for_Large_Displays_using_a_Passive_Wand_Tracked_in_3D)
- ["Matrix: a realtime object identification and registration method for augmented reality"](https://nalinc.github.io/the-midnight-paper/papers/Matrix:A_realtime_object_identification_and_registration_method_for_augmented_reality)
- ["Designable Visual Markers"](https://nalinc.github.io/the-midnight-paper/papers/Designable_Visual_Markers)
- ["FrameWire: A Tool for Automatically Extracting Interaction Logic from Paper Prototyping Tests"](https://nalinc.github.io/the-midnight-paper/papers/FrameWire:A_Tool_for_Automatically_Extracting_Interaction_Logic_from_Paper_Prototyping_Tests)
- ["Video Puppetry: A Performative Interface for Cutout Animation"](https://nalinc.github.io/the-midnight-paper/papers/VideoPuppetry:A_Performative_Interface_for_Cutout_Animation)
- ["Video Browsing by Direct Manipulation"](https://nalinc.github.io/the-midnight-paper/papers/Video_Browsing_by_Direct_Manipulation)
- ["RetroDepth: 3D Silhouette Sensing for High-Precision Input On and Above Physical Surfaces"](https://nalinc.github.io/the-midnight-paper/papers/RetroDepth:3D_Silhouette_Sensing_for_High-Precision_Input_On_and_Above_Physical_Surfaces)
- ["TouchLight - An Imaging Touch Screen and Display for Gesture-Based Interaction"](https://nalinc.github.io/the-midnight-paper/papers/TouchLight:An_Imaging_Touch_Screen_and_Display_for_Gesture_Based_Interaction)
- ["Visual Touchpad: A Two-handed Gestural Input Device"](https://nalinc.github.io/the-midnight-paper/papers/VisualTouchpad:A_Two_handed_Gestural_Input_Device)
- ["Palpebrae Superioris: Exploring the Design Space of Eyelid Gestures"](https://nalinc.github.io/the-midnight-paper/papers/PalpebraeSuperioris:Exploring_the_Design_Space_of_Eyelid_Gestures)
- ["EyeScout: Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays"](https://nalinc.github.io/the-midnight-paper/papers/EyeScout)
- ["RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-camera Units"](https://nalinc.github.io/the-midnight-paper/papers/RoomAlive)
- ["The Office of the Future : A Unified Approach to Image-Based Modeling and Spatially Immersive Displays"](https://nalinc.github.io/the-midnight-paper/papers/The_Office_of_the_Future)
- ["WorldKit: Rapid and Easy Creation of Ad-hoc Interactive Applications on Everyday Surfaces"](https://nalinc.github.io/the-midnight-paper/papers/WorldKit)
- ["Prefab: implementing advanced behaviors using pixel-based reverse engineering of interface structure"](https://nalinc.github.io/the-midnight-paper/papers/Prefab)
- ["MixFab: A Mixed-reality Environment for Personal Fabrication"](https://nalinc.github.io/the-midnight-paper/papers/MixFab)
- ["Tohme: detecting curb ramps in google street view using crowdsourcing, computer vision, and machine learning"](https://nalinc.github.io/the-midnight-paper/papers/Tohme)


**Human AI Interaction**
- ["Like Having a Really bad PA": The Gulf between User Expectation and Experience of Conversational Agents](https://nalinc.github.io/the-midnight-paper/papers/The_Gulf_between_User_Expectation_and_Experience_of_Conversational_Agents)
- "What do people expect from robots?"
- "Becoming Wikipedian: transformation of participation in a collaborative online encyclopedia"
- "What is a robot companion - Friend, assistant or butler?"
- "Toys that Listen: A Study of Parents, Children, and Internet-Connected Toys"
- ["How Much Information? Effects of Transparency on Trust in an Algorithmic Interface"](https://nalinc.github.io/the-midnight-paper/papers/How_Much_Information_Effects_of_Transparency_on_Trust_in_an_Algorithmic_Interface)
- ["Priming Drivers before Handover in Semi-Autonomous Cars"](https://nalinc.github.io/the-midnight-paper/papers/Priming_Drivers_before_Handover_in_Semi-Autonomous_Cars)
- ["Enabling Building Service Robots to Guide Blind People"](https://nalinc.github.io/the-midnight-paper/papers/Enabling_Building_Service_Robots_to_Guide_Blind_People)
- ["Imagining Artificial Intelligence Applications with People with Visual Disabilities using Tactile Ideation"](https://nalinc.github.io/the-midnight-paper/papers/Imagining_Artificial_Intelligence_Applications_with_People_with_Visual_Disabilities_using_Tactile_Ideation)
- "Whole-Home Gesture Recognition Using Wireless Signals"



# Template
```
## PaperTitle

#### Authors: 
#### Conference
#### Keywords

#### Strength
#### Weakness
#### Future Work
---
**What did authors learn:**
-
**Relevant work:**
-
```
